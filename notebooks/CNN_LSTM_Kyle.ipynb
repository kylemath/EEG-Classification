{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_LSTM_2_Kyle.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kylemath/EEG-Classification/blob/master/notebooks/CNN_LSTM_Kyle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "cH7KRd8ZZPMd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## CNN_LSTM_Kyle\n",
        "\n",
        "Goal is to make a CNN LSTM stack that processes EEG trials as input and predicts binary category as output.\n",
        "\n",
        "\n",
        "Strategy:\n",
        "* Current code uses 25,000 examples of 100 long sentences in two categories, \n",
        "* Then is tested on 25000 sequences as well\n",
        "* Instead try 250 by 100 data point long ERP for each trial \n",
        "* Predict target vs standard on any EEG dataset (start with Nathan skateboard data)\n",
        "* Predict attend left vs attend right on muse 375 data\n",
        "\n",
        "Using: \n",
        "* https://github.com/keras-team/keras/blob/master/examples/imdb_cnn_lstm.py\n",
        "* https://github.com/pbashivan/EEGLearn\n",
        "* https://github.com/tevisgehr/EEG-Classification\n",
        "\n",
        "Resources:\n",
        "*   http://proceedings.mlr.press/v56/Thodoroff16.pdf\n",
        "*   https://arxiv.org/abs/1511.06448\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "VXe-o8XtG9ki",
        "colab_type": "code",
        "outputId": "fbea4100-c3d5-4ebe-c036-1bd20c840d04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/tevisgehr/EEG-Classification.git\n",
        "%cd EEG-Classification"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'EEG-Classification'...\n",
            "remote: Enumerating objects: 201, done.\u001b[K\n",
            "remote: Total 201 (delta 0), reused 0 (delta 0), pack-reused 201\u001b[K\n",
            "Receiving objects: 100% (201/201), 13.62 MiB | 6.17 MiB/s, done.\n",
            "Resolving deltas: 100% (99/99), done.\n",
            "/content/EEG-Classification\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Q_NcbDGsG6_A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from eeg_learn_functions import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sSvKReofG6_I",
        "colab_type": "code",
        "outputId": "3e466b4e-c3b7-43e2-d054-2e59c08c6bd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import scipy.stats as scs\n",
        "import re\n",
        "from numpy import genfromtxt\n",
        "\n",
        "%matplotlib inline\n",
        "plt.style.use('ggplot')\n",
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
        "plt.rcParams[\"figure.figsize\"] = (12,12)\n",
        "pd.options.display.max_columns = None\n",
        "pd.options.display.precision = 4"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>.container { width:100% !important; }</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "gSGBQBcXG6_Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Brainwave Frequencies:\n",
        "Gamma, 30 to 50 Hz.  \n",
        "Beta, 14 to 30 Hz.  \n",
        "Alpha, 8 to 14 Hz.  \n",
        "Theta, 4 to 8 Hz.  \n",
        "Delta, 0.1 to 4 Hz.  "
      ]
    },
    {
      "metadata": {
        "id": "w8_YWq7dG6_R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Changing Bin Size: \n",
        "https://stackoverflow.com/questions/25735153/plotting-a-fast-fourier-transform-in-python  \n",
        "(Search for 'bin')"
      ]
    },
    {
      "metadata": {
        "id": "j5mm0SyFG6_T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "An EEG processing library:  \n",
        "https://github.com/pbashivan/EEGLearn"
      ]
    },
    {
      "metadata": {
        "id": "8buBuEsbG6_U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "theta = (4,8)\n",
        "alpha = (8,12)\n",
        "beta = (12,40)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "asaiRYdZG6_X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_fft(snippet):\n",
        "    Fs = 128.0;  # sampling rate\n",
        "    #Ts = len(snippet)/Fs/Fs; # sampling interval\n",
        "    snippet_time = len(snippet)/Fs\n",
        "    Ts = 1.0/Fs; # sampling interval\n",
        "    t = np.arange(0,snippet_time,Ts) # time vector\n",
        "\n",
        "    # ff = 5;   # frequency of the signal\n",
        "    # y = np.sin(2*np.pi*ff*t)\n",
        "    y = snippet\n",
        "#     print('Ts: ',Ts)\n",
        "#     print(t)\n",
        "#     print(y.shape)\n",
        "    n = len(y) # length of the signal\n",
        "    k = np.arange(n)\n",
        "    T = n/Fs\n",
        "    frq = k/T # two sides frequency range\n",
        "    frq = frq[range(n//2)] # one side frequency range\n",
        "\n",
        "    Y = np.fft.fft(y)/n # fft computing and normalization\n",
        "    Y = Y[range(n//2)]\n",
        "    #Added in: (To remove bias.)\n",
        "    #Y[0] = 0\n",
        "    return frq,abs(Y)\n",
        "#f,Y = get_fft(np.hanning(len(snippet))*snippet)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uNwMQSrQG6_d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def theta_alpha_beta_averages(f,Y):\n",
        "    theta_range = (4,8)\n",
        "    alpha_range = (8,12)\n",
        "    beta_range = (12,40)\n",
        "    theta = Y[(f>theta_range[0]) & (f<=theta_range[1])].mean()\n",
        "    alpha = Y[(f>alpha_range[0]) & (f<=alpha_range[1])].mean()\n",
        "    beta = Y[(f>beta_range[0]) & (f<=beta_range[1])].mean()\n",
        "    return theta, alpha, beta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vbOLRN1ZG6_h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_steps(samples,frame_duration,overlap):\n",
        "    '''\n",
        "    in:\n",
        "    samples - number of samples in the session\n",
        "    frame_duration - frame duration in seconds \n",
        "    overlap - float fraction of frame to overlap in range (0,1)\n",
        "    \n",
        "    out: list of tuple ranges\n",
        "    '''\n",
        "    #steps = np.arange(0,len(df),frame_length)\n",
        "    Fs = 128\n",
        "    i = 0\n",
        "    intervals = []\n",
        "    samples_per_frame = Fs * frame_duration\n",
        "    while i+samples_per_frame <= samples:\n",
        "        intervals.append((i,i+samples_per_frame))\n",
        "        i = i + samples_per_frame - int(samples_per_frame*overlap)\n",
        "    return intervals"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "beuByNJWG6_l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_frames(df,frame_duration):\n",
        "    '''\n",
        "    in: dataframe or array with all channels, frame duration in seconds\n",
        "    out: array of theta, alpha, beta averages for each probe for each time step\n",
        "        shape: (n-frames,m-probes,k-brainwave bands)\n",
        "    '''\n",
        "    Fs = 128.0\n",
        "    frame_length = Fs*frame_duration\n",
        "    frames = []\n",
        "    steps = make_steps(len(df),frame_duration,overlap)\n",
        "    for i,_ in enumerate(steps):\n",
        "        frame = []\n",
        "        if i == 0:\n",
        "            continue\n",
        "        else:\n",
        "            for channel in df.columns:\n",
        "                snippet = np.array(df.loc[steps[i][0]:steps[i][1],int(channel)])\n",
        "                f,Y =  get_fft(snippet)\n",
        "                theta, alpha, beta = theta_alpha_beta_averages(f,Y)\n",
        "                frame.append([theta, alpha, beta])\n",
        "            \n",
        "        frames.append(frame)\n",
        "    return np.array(frames)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SjRzRkWCG6_p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "locs_2d = [(-2.0,4.0),\n",
        "           (2.0,4.0),\n",
        "           (-1.0,3.0),\n",
        "           (1.0,3.0),\n",
        "           (-3.0,3.0),\n",
        "           (3.0,3.0),\n",
        "           (-2.0,2.0),\n",
        "           (2.0,2.0),\n",
        "           (-2.0,-2.0),\n",
        "           (2.0,-2.0),\n",
        "           (-4.0,1.0),\n",
        "           (4.0,1.0),\n",
        "           (-1.0,-3.0),\n",
        "           (1.0,-3.0)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oI7oDkqdG6_t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_data_pipeline(file_names,labels,image_size,frame_duration,overlap):\n",
        "    '''\n",
        "    IN: \n",
        "    file_names - list of strings for each input file (one for each subject)\n",
        "    labels - list of labels for each\n",
        "    image_size - int size of output images in form (x, x)\n",
        "    frame_duration - time length of each frame (seconds)\n",
        "    overlap - float fraction of frame to overlap in range (0,1)\n",
        "    \n",
        "    OUT:\n",
        "    X: np array of frames (unshuffled)\n",
        "    y: np array of label for each frame (1 or 0)\n",
        "    '''\n",
        "    ##################################\n",
        "    ###Still need to do the overlap###!!!\n",
        "    ##################################\n",
        "    \n",
        "    Fs = 128.0   #sampling rate\n",
        "    frame_length = Fs * frame_duration\n",
        "    \n",
        "    print('Generating training data...')\n",
        "    \n",
        "    \n",
        "    for i, file in enumerate(file_names):\n",
        "        print ('Processing session: ',file, '. (',i+1,' of ',len(file_names),')')\n",
        "        data = genfromtxt(file, delimiter=',').T\n",
        "        df = pd.DataFrame(data)\n",
        "        \n",
        "        X_0 = make_frames(df,frame_duration)\n",
        "        #steps = np.arange(0,len(df),frame_length)\n",
        "        X_1 = X_0.reshape(len(X_0),14*3)\n",
        "        \n",
        "        images = gen_images(np.array(locs_2d),X_1, image_size, normalize=False)\n",
        "        images = np.swapaxes(images, 1, 3) \n",
        "        print(len(images), ' frames generated with label ', labels[i], '.')\n",
        "        print('\\n')\n",
        "        if i == 0:\n",
        "            X = images\n",
        "            y = np.ones(len(images))*labels[0]\n",
        "        else:\n",
        "            X = np.concatenate((X,images),axis = 0)\n",
        "            y = np.concatenate((y,np.ones(len(images))*labels[i]),axis = 0)\n",
        "        \n",
        "        \n",
        "    return X,np.array(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "uhinfgcdG6_z",
        "colab_type": "code",
        "outputId": "062b30e8-c633-4e38-a059-40fa32a551e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "file_names = ['data/ML102_KS.csv',\n",
        "              'data/ML102_US.csv']\n",
        "labels = [1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0]\n",
        "image_size = 28\n",
        "frame_duration = 1.0\n",
        "overlap = 0.5\n",
        "X, y = make_data_pipeline(file_names,labels,image_size,frame_duration,overlap)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating training data...\n",
            "Processing session:  data/ML102_KS.csv . ( 1  of  2 )\n",
            "222  frames generated with label  1 .\n",
            "\n",
            "\n",
            "Processing session:  data/ML102_US.csv . ( 2  of  2 )\n",
            "218  frames generated with label  0 .\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bAr90EeaG6_2",
        "colab_type": "code",
        "outputId": "dd0f4367-cf53-45a2-d413-b2e8bcece379",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(458, 28, 28, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "jP90d308G6_7",
        "colab_type": "code",
        "outputId": "60834ee8-56d8-44be-96f8-20d5a290b9e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "y.shape\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(458,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "metadata": {
        "id": "7j-lctn_G7AA",
        "colab_type": "code",
        "outputId": "6d60725c-e470-48b4-b642-799ae26a0654",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(X[10]);\n",
        "plt.colorbar()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.colorbar.Colorbar at 0x7f02dd821a58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAASoAAAD8CAYAAADAKumpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEQlPiUpEwlOiEpHwlKhEJDwlKhEJ7/8BZKNahmxn+PwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "u5FhwRdqG7AF",
        "colab_type": "code",
        "outputId": "387eaac7-6c9e-4f23-995e-73a11cda64e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        }
      },
      "cell_type": "code",
      "source": [
        "# Two subplots, the axes array is 1-d\n",
        "f, axarr = plt.subplots(2,2, figsize = (8,8))\n",
        "axarr[0][0].set_title('Known Skill')\n",
        "axarr[0][0].imshow(X[0])\n",
        "axarr[1][0].imshow(X[1])\n",
        "\n",
        "axarr[0][1].set_title('Unknown Skill')\n",
        "axarr[0][1].imshow(X[20])\n",
        "axarr[1][1].imshow(X[21])\n",
        "\n",
        ";"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdkAAAHjCAYAAABvpmXJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x576 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "U1-kvK4gG7AL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train_temp, y_test_temp = train_test_split(X, y, test_size=0.20,shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zgSrs-0jG7AO",
        "colab_type": "code",
        "outputId": "9c435743-e400-409c-f080-e8888228a283",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "print(y_train_temp.shape)\n",
        "print(x_train.shape)\n",
        "print(y_test_temp.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(352,)\n",
            "(352, 28, 28, 3)\n",
            "(88,)\n",
            "(88, 28, 28, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CEP1RJpNG7AV",
        "colab_type": "code",
        "outputId": "01848b7c-4a42-4c09-9022-57e83e15e01d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "#x_train /= 255\n",
        "#x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "input_shape = (img_rows, img_cols, 3)\n",
        "\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (352, 28, 28, 3)\n",
            "352 train samples\n",
            "88 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "JxjbgyC5G7AZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import LSTM, ConvLSTM2D\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 2\n",
        "epochs = 400\n",
        "lstm_output_size = 10\n",
        "\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train_temp, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test_temp, num_classes)\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "#model.add(ConvLSTM2D(lstm_output_size,input_shape=input_shape,kernel_size=3))\n",
        "#model.add(Dense(num_classes))\n",
        "#model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), padding='same',input_shape=input_shape))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "# initiate RMSprop optimizer\n",
        "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
        "\n",
        "# Let's train the model using RMSprop\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xyz97yRsG7Ad",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 13651
        },
        "outputId": "76009ce4-6396-4aaa-deff-bf7a60e8f5c3"
      },
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=(x_test, y_test),\n",
        "          shuffle=True)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 352 samples, validate on 88 samples\n",
            "Epoch 1/400\n",
            "352/352 [==============================] - 1s 2ms/step - loss: 0.7196 - acc: 0.4886 - val_loss: 0.6895 - val_acc: 0.5000\n",
            "Epoch 2/400\n",
            "352/352 [==============================] - 0s 152us/step - loss: 0.6847 - acc: 0.5284 - val_loss: 0.6868 - val_acc: 0.5114\n",
            "Epoch 3/400\n",
            "352/352 [==============================] - 0s 155us/step - loss: 0.6835 - acc: 0.5426 - val_loss: 0.6846 - val_acc: 0.5455\n",
            "Epoch 4/400\n",
            "352/352 [==============================] - 0s 154us/step - loss: 0.6776 - acc: 0.5483 - val_loss: 0.6858 - val_acc: 0.5455\n",
            "Epoch 5/400\n",
            "352/352 [==============================] - 0s 155us/step - loss: 0.6727 - acc: 0.5909 - val_loss: 0.6800 - val_acc: 0.6136\n",
            "Epoch 6/400\n",
            "352/352 [==============================] - 0s 139us/step - loss: 0.6646 - acc: 0.6392 - val_loss: 0.6772 - val_acc: 0.6023\n",
            "Epoch 7/400\n",
            "352/352 [==============================] - 0s 138us/step - loss: 0.6677 - acc: 0.5909 - val_loss: 0.6747 - val_acc: 0.5568\n",
            "Epoch 8/400\n",
            "352/352 [==============================] - 0s 146us/step - loss: 0.6614 - acc: 0.5795 - val_loss: 0.6721 - val_acc: 0.6932\n",
            "Epoch 9/400\n",
            "352/352 [==============================] - 0s 157us/step - loss: 0.6583 - acc: 0.6307 - val_loss: 0.6696 - val_acc: 0.7045\n",
            "Epoch 10/400\n",
            "352/352 [==============================] - 0s 134us/step - loss: 0.6563 - acc: 0.6477 - val_loss: 0.6674 - val_acc: 0.5455\n",
            "Epoch 11/400\n",
            "352/352 [==============================] - 0s 140us/step - loss: 0.6466 - acc: 0.5994 - val_loss: 0.6655 - val_acc: 0.7045\n",
            "Epoch 12/400\n",
            "352/352 [==============================] - 0s 135us/step - loss: 0.6449 - acc: 0.6591 - val_loss: 0.6619 - val_acc: 0.6477\n",
            "Epoch 13/400\n",
            "352/352 [==============================] - 0s 133us/step - loss: 0.6383 - acc: 0.6761 - val_loss: 0.6594 - val_acc: 0.6477\n",
            "Epoch 14/400\n",
            "352/352 [==============================] - 0s 137us/step - loss: 0.6408 - acc: 0.6591 - val_loss: 0.6569 - val_acc: 0.6477\n",
            "Epoch 15/400\n",
            "352/352 [==============================] - 0s 145us/step - loss: 0.6254 - acc: 0.7017 - val_loss: 0.6537 - val_acc: 0.6591\n",
            "Epoch 16/400\n",
            "352/352 [==============================] - 0s 132us/step - loss: 0.6262 - acc: 0.6705 - val_loss: 0.6512 - val_acc: 0.6705\n",
            "Epoch 17/400\n",
            "352/352 [==============================] - 0s 129us/step - loss: 0.6295 - acc: 0.6705 - val_loss: 0.6516 - val_acc: 0.7273\n",
            "Epoch 18/400\n",
            "352/352 [==============================] - 0s 131us/step - loss: 0.6205 - acc: 0.6932 - val_loss: 0.6485 - val_acc: 0.7159\n",
            "Epoch 19/400\n",
            "352/352 [==============================] - 0s 130us/step - loss: 0.6136 - acc: 0.6733 - val_loss: 0.6479 - val_acc: 0.7273\n",
            "Epoch 20/400\n",
            "352/352 [==============================] - 0s 134us/step - loss: 0.6117 - acc: 0.6989 - val_loss: 0.6417 - val_acc: 0.6705\n",
            "Epoch 21/400\n",
            "352/352 [==============================] - 0s 137us/step - loss: 0.6151 - acc: 0.6648 - val_loss: 0.6387 - val_acc: 0.6818\n",
            "Epoch 22/400\n",
            "352/352 [==============================] - 0s 141us/step - loss: 0.6152 - acc: 0.6420 - val_loss: 0.6364 - val_acc: 0.6705\n",
            "Epoch 23/400\n",
            "352/352 [==============================] - 0s 134us/step - loss: 0.6074 - acc: 0.7017 - val_loss: 0.6352 - val_acc: 0.6364\n",
            "Epoch 24/400\n",
            "352/352 [==============================] - 0s 139us/step - loss: 0.6010 - acc: 0.6903 - val_loss: 0.6355 - val_acc: 0.7159\n",
            "Epoch 25/400\n",
            "352/352 [==============================] - 0s 143us/step - loss: 0.5970 - acc: 0.7102 - val_loss: 0.6289 - val_acc: 0.6932\n",
            "Epoch 26/400\n",
            "352/352 [==============================] - 0s 138us/step - loss: 0.5970 - acc: 0.6676 - val_loss: 0.6295 - val_acc: 0.7386\n",
            "Epoch 27/400\n",
            "352/352 [==============================] - 0s 141us/step - loss: 0.6002 - acc: 0.6847 - val_loss: 0.6218 - val_acc: 0.7045\n",
            "Epoch 28/400\n",
            "352/352 [==============================] - 0s 132us/step - loss: 0.5888 - acc: 0.7074 - val_loss: 0.6191 - val_acc: 0.6932\n",
            "Epoch 29/400\n",
            "352/352 [==============================] - 0s 149us/step - loss: 0.5880 - acc: 0.6705 - val_loss: 0.6247 - val_acc: 0.7500\n",
            "Epoch 30/400\n",
            "352/352 [==============================] - 0s 142us/step - loss: 0.6000 - acc: 0.6818 - val_loss: 0.6231 - val_acc: 0.7614\n",
            "Epoch 31/400\n",
            "352/352 [==============================] - 0s 142us/step - loss: 0.5791 - acc: 0.7244 - val_loss: 0.6109 - val_acc: 0.6591\n",
            "Epoch 32/400\n",
            "352/352 [==============================] - 0s 134us/step - loss: 0.5800 - acc: 0.6790 - val_loss: 0.6109 - val_acc: 0.6932\n",
            "Epoch 33/400\n",
            "352/352 [==============================] - 0s 143us/step - loss: 0.5782 - acc: 0.7017 - val_loss: 0.6070 - val_acc: 0.6932\n",
            "Epoch 34/400\n",
            "352/352 [==============================] - 0s 131us/step - loss: 0.5686 - acc: 0.7045 - val_loss: 0.6015 - val_acc: 0.6932\n",
            "Epoch 35/400\n",
            "352/352 [==============================] - 0s 141us/step - loss: 0.5803 - acc: 0.6960 - val_loss: 0.6101 - val_acc: 0.7500\n",
            "Epoch 36/400\n",
            "352/352 [==============================] - 0s 135us/step - loss: 0.5770 - acc: 0.7188 - val_loss: 0.6014 - val_acc: 0.7045\n",
            "Epoch 37/400\n",
            "352/352 [==============================] - 0s 145us/step - loss: 0.5720 - acc: 0.7074 - val_loss: 0.6050 - val_acc: 0.7386\n",
            "Epoch 38/400\n",
            "352/352 [==============================] - 0s 143us/step - loss: 0.5741 - acc: 0.6903 - val_loss: 0.5946 - val_acc: 0.6818\n",
            "Epoch 39/400\n",
            "352/352 [==============================] - 0s 157us/step - loss: 0.5668 - acc: 0.6932 - val_loss: 0.6072 - val_acc: 0.7386\n",
            "Epoch 40/400\n",
            "352/352 [==============================] - 0s 140us/step - loss: 0.5687 - acc: 0.7074 - val_loss: 0.5927 - val_acc: 0.7159\n",
            "Epoch 41/400\n",
            "352/352 [==============================] - 0s 135us/step - loss: 0.5662 - acc: 0.6989 - val_loss: 0.5971 - val_acc: 0.7500\n",
            "Epoch 42/400\n",
            "352/352 [==============================] - 0s 137us/step - loss: 0.5758 - acc: 0.7216 - val_loss: 0.5963 - val_acc: 0.7500\n",
            "Epoch 43/400\n",
            "352/352 [==============================] - 0s 132us/step - loss: 0.5578 - acc: 0.7188 - val_loss: 0.5833 - val_acc: 0.7045\n",
            "Epoch 44/400\n",
            "352/352 [==============================] - 0s 135us/step - loss: 0.5533 - acc: 0.7131 - val_loss: 0.5954 - val_acc: 0.7386\n",
            "Epoch 45/400\n",
            "352/352 [==============================] - 0s 144us/step - loss: 0.5754 - acc: 0.6761 - val_loss: 0.5809 - val_acc: 0.7045\n",
            "Epoch 46/400\n",
            "352/352 [==============================] - 0s 145us/step - loss: 0.5582 - acc: 0.7301 - val_loss: 0.5782 - val_acc: 0.7159\n",
            "Epoch 47/400\n",
            "352/352 [==============================] - 0s 133us/step - loss: 0.5607 - acc: 0.6960 - val_loss: 0.5763 - val_acc: 0.7045\n",
            "Epoch 48/400\n",
            "352/352 [==============================] - 0s 134us/step - loss: 0.5564 - acc: 0.6932 - val_loss: 0.5770 - val_acc: 0.7159\n",
            "Epoch 49/400\n",
            "352/352 [==============================] - 0s 130us/step - loss: 0.5534 - acc: 0.7188 - val_loss: 0.5738 - val_acc: 0.7273\n",
            "Epoch 50/400\n",
            "352/352 [==============================] - 0s 145us/step - loss: 0.5447 - acc: 0.7386 - val_loss: 0.5712 - val_acc: 0.7045\n",
            "Epoch 51/400\n",
            "352/352 [==============================] - 0s 141us/step - loss: 0.5474 - acc: 0.7244 - val_loss: 0.5715 - val_acc: 0.7386\n",
            "Epoch 52/400\n",
            "352/352 [==============================] - 0s 133us/step - loss: 0.5535 - acc: 0.7244 - val_loss: 0.5740 - val_acc: 0.7159\n",
            "Epoch 53/400\n",
            "352/352 [==============================] - 0s 133us/step - loss: 0.5570 - acc: 0.7017 - val_loss: 0.5659 - val_acc: 0.7045\n",
            "Epoch 54/400\n",
            "352/352 [==============================] - 0s 146us/step - loss: 0.5432 - acc: 0.7273 - val_loss: 0.5771 - val_acc: 0.7500\n",
            "Epoch 55/400\n",
            "352/352 [==============================] - 0s 142us/step - loss: 0.5583 - acc: 0.7102 - val_loss: 0.5751 - val_acc: 0.7500\n",
            "Epoch 56/400\n",
            "352/352 [==============================] - 0s 132us/step - loss: 0.5500 - acc: 0.7102 - val_loss: 0.5634 - val_acc: 0.7273\n",
            "Epoch 57/400\n",
            "352/352 [==============================] - 0s 141us/step - loss: 0.5433 - acc: 0.7244 - val_loss: 0.5603 - val_acc: 0.7045\n",
            "Epoch 58/400\n",
            "352/352 [==============================] - 0s 132us/step - loss: 0.5431 - acc: 0.7244 - val_loss: 0.5621 - val_acc: 0.7386\n",
            "Epoch 59/400\n",
            "352/352 [==============================] - 0s 144us/step - loss: 0.5435 - acc: 0.7273 - val_loss: 0.5602 - val_acc: 0.7386\n",
            "Epoch 60/400\n",
            "352/352 [==============================] - 0s 141us/step - loss: 0.5404 - acc: 0.7330 - val_loss: 0.5681 - val_acc: 0.7500\n",
            "Epoch 61/400\n",
            "352/352 [==============================] - 0s 140us/step - loss: 0.5499 - acc: 0.7301 - val_loss: 0.5546 - val_acc: 0.7159\n",
            "Epoch 62/400\n",
            "352/352 [==============================] - 0s 138us/step - loss: 0.5439 - acc: 0.7301 - val_loss: 0.5541 - val_acc: 0.7386\n",
            "Epoch 63/400\n",
            "352/352 [==============================] - 0s 146us/step - loss: 0.5527 - acc: 0.7244 - val_loss: 0.5630 - val_acc: 0.7386\n",
            "Epoch 64/400\n",
            "352/352 [==============================] - 0s 135us/step - loss: 0.5394 - acc: 0.7386 - val_loss: 0.5607 - val_acc: 0.7500\n",
            "Epoch 65/400\n",
            "352/352 [==============================] - 0s 139us/step - loss: 0.5460 - acc: 0.7301 - val_loss: 0.5517 - val_acc: 0.7159\n",
            "Epoch 66/400\n",
            "352/352 [==============================] - 0s 133us/step - loss: 0.5334 - acc: 0.7188 - val_loss: 0.6081 - val_acc: 0.7045\n",
            "Epoch 67/400\n",
            "352/352 [==============================] - 0s 143us/step - loss: 0.5593 - acc: 0.7045 - val_loss: 0.5578 - val_acc: 0.7386\n",
            "Epoch 68/400\n",
            "352/352 [==============================] - 0s 134us/step - loss: 0.5315 - acc: 0.7358 - val_loss: 0.5570 - val_acc: 0.7614\n",
            "Epoch 69/400\n",
            "352/352 [==============================] - 0s 133us/step - loss: 0.5366 - acc: 0.7585 - val_loss: 0.5492 - val_acc: 0.7273\n",
            "Epoch 70/400\n",
            "352/352 [==============================] - 0s 160us/step - loss: 0.5350 - acc: 0.7472 - val_loss: 0.5514 - val_acc: 0.7386\n",
            "Epoch 71/400\n",
            "352/352 [==============================] - 0s 128us/step - loss: 0.5426 - acc: 0.7102 - val_loss: 0.5508 - val_acc: 0.7386\n",
            "Epoch 72/400\n",
            "352/352 [==============================] - 0s 133us/step - loss: 0.5424 - acc: 0.7301 - val_loss: 0.5471 - val_acc: 0.7273\n",
            "Epoch 73/400\n",
            "352/352 [==============================] - 0s 134us/step - loss: 0.5340 - acc: 0.7330 - val_loss: 0.5727 - val_acc: 0.7386\n",
            "Epoch 74/400\n",
            "352/352 [==============================] - 0s 148us/step - loss: 0.5456 - acc: 0.7159 - val_loss: 0.5602 - val_acc: 0.7500\n",
            "Epoch 75/400\n",
            "352/352 [==============================] - 0s 140us/step - loss: 0.5279 - acc: 0.7443 - val_loss: 0.5455 - val_acc: 0.7273\n",
            "Epoch 76/400\n",
            "352/352 [==============================] - 0s 140us/step - loss: 0.5327 - acc: 0.7386 - val_loss: 0.5451 - val_acc: 0.7727\n",
            "Epoch 77/400\n",
            "352/352 [==============================] - 0s 133us/step - loss: 0.5361 - acc: 0.7244 - val_loss: 0.5428 - val_acc: 0.7273\n",
            "Epoch 78/400\n",
            "352/352 [==============================] - 0s 137us/step - loss: 0.5267 - acc: 0.7443 - val_loss: 0.5435 - val_acc: 0.7273\n",
            "Epoch 79/400\n",
            "352/352 [==============================] - 0s 138us/step - loss: 0.5277 - acc: 0.7358 - val_loss: 0.5543 - val_acc: 0.7727\n",
            "Epoch 80/400\n",
            "352/352 [==============================] - 0s 136us/step - loss: 0.5256 - acc: 0.7472 - val_loss: 0.5411 - val_acc: 0.7386\n",
            "Epoch 81/400\n",
            "352/352 [==============================] - 0s 135us/step - loss: 0.5461 - acc: 0.7443 - val_loss: 0.5399 - val_acc: 0.7386\n",
            "Epoch 82/400\n",
            "352/352 [==============================] - 0s 136us/step - loss: 0.5280 - acc: 0.7443 - val_loss: 0.5394 - val_acc: 0.7500\n",
            "Epoch 83/400\n",
            "352/352 [==============================] - 0s 138us/step - loss: 0.5311 - acc: 0.7159 - val_loss: 0.5450 - val_acc: 0.7614\n",
            "Epoch 84/400\n",
            "352/352 [==============================] - 0s 143us/step - loss: 0.5256 - acc: 0.7557 - val_loss: 0.5378 - val_acc: 0.7386\n",
            "Epoch 85/400\n",
            "352/352 [==============================] - 0s 140us/step - loss: 0.5249 - acc: 0.7443 - val_loss: 0.5442 - val_acc: 0.7614\n",
            "Epoch 86/400\n",
            "352/352 [==============================] - 0s 132us/step - loss: 0.5325 - acc: 0.7415 - val_loss: 0.5405 - val_acc: 0.7727\n",
            "Epoch 87/400\n",
            "352/352 [==============================] - 0s 135us/step - loss: 0.5364 - acc: 0.7443 - val_loss: 0.5371 - val_acc: 0.7614\n",
            "Epoch 88/400\n",
            "352/352 [==============================] - 0s 141us/step - loss: 0.5291 - acc: 0.7415 - val_loss: 0.5432 - val_acc: 0.7614\n",
            "Epoch 89/400\n",
            "352/352 [==============================] - 0s 134us/step - loss: 0.5191 - acc: 0.7358 - val_loss: 0.5484 - val_acc: 0.7841\n",
            "Epoch 90/400\n",
            "352/352 [==============================] - 0s 151us/step - loss: 0.5236 - acc: 0.7386 - val_loss: 0.5372 - val_acc: 0.7386\n",
            "Epoch 91/400\n",
            "352/352 [==============================] - 0s 138us/step - loss: 0.5222 - acc: 0.7244 - val_loss: 0.5353 - val_acc: 0.7386\n",
            "Epoch 92/400\n",
            "352/352 [==============================] - 0s 133us/step - loss: 0.5121 - acc: 0.7585 - val_loss: 0.5435 - val_acc: 0.7614\n",
            "Epoch 93/400\n",
            "352/352 [==============================] - 0s 140us/step - loss: 0.5206 - acc: 0.7557 - val_loss: 0.5367 - val_acc: 0.7500\n",
            "Epoch 94/400\n",
            "352/352 [==============================] - 0s 142us/step - loss: 0.5225 - acc: 0.7528 - val_loss: 0.5338 - val_acc: 0.7386\n",
            "Epoch 95/400\n",
            "352/352 [==============================] - 0s 141us/step - loss: 0.5168 - acc: 0.7415 - val_loss: 0.5365 - val_acc: 0.7500\n",
            "Epoch 96/400\n",
            "352/352 [==============================] - 0s 132us/step - loss: 0.5211 - acc: 0.7443 - val_loss: 0.5319 - val_acc: 0.7386\n",
            "Epoch 97/400\n",
            "352/352 [==============================] - 0s 140us/step - loss: 0.5273 - acc: 0.7301 - val_loss: 0.5388 - val_acc: 0.7614\n",
            "Epoch 98/400\n",
            "352/352 [==============================] - 0s 135us/step - loss: 0.5166 - acc: 0.7528 - val_loss: 0.5301 - val_acc: 0.7386\n",
            "Epoch 99/400\n",
            "352/352 [==============================] - 0s 136us/step - loss: 0.5132 - acc: 0.7614 - val_loss: 0.5303 - val_acc: 0.7386\n",
            "Epoch 100/400\n",
            "352/352 [==============================] - 0s 142us/step - loss: 0.5147 - acc: 0.7756 - val_loss: 0.5294 - val_acc: 0.7841\n",
            "Epoch 101/400\n",
            "352/352 [==============================] - 0s 132us/step - loss: 0.5289 - acc: 0.7131 - val_loss: 0.5371 - val_acc: 0.7727\n",
            "Epoch 102/400\n",
            "352/352 [==============================] - 0s 142us/step - loss: 0.5083 - acc: 0.7727 - val_loss: 0.5303 - val_acc: 0.7841\n",
            "Epoch 103/400\n",
            "352/352 [==============================] - 0s 135us/step - loss: 0.5204 - acc: 0.7415 - val_loss: 0.5316 - val_acc: 0.7500\n",
            "Epoch 104/400\n",
            "352/352 [==============================] - 0s 136us/step - loss: 0.5140 - acc: 0.7443 - val_loss: 0.5391 - val_acc: 0.7727\n",
            "Epoch 105/400\n",
            "352/352 [==============================] - 0s 129us/step - loss: 0.5342 - acc: 0.7386 - val_loss: 0.5282 - val_acc: 0.7386\n",
            "Epoch 106/400\n",
            "352/352 [==============================] - 0s 137us/step - loss: 0.5148 - acc: 0.7557 - val_loss: 0.5367 - val_acc: 0.7841\n",
            "Epoch 107/400\n",
            "352/352 [==============================] - 0s 134us/step - loss: 0.5183 - acc: 0.7500 - val_loss: 0.5521 - val_acc: 0.7500\n",
            "Epoch 108/400\n",
            "352/352 [==============================] - 0s 129us/step - loss: 0.5044 - acc: 0.7614 - val_loss: 0.5270 - val_acc: 0.7841\n",
            "Epoch 109/400\n",
            "352/352 [==============================] - 0s 140us/step - loss: 0.5120 - acc: 0.7727 - val_loss: 0.5272 - val_acc: 0.7841\n",
            "Epoch 110/400\n",
            "352/352 [==============================] - 0s 137us/step - loss: 0.5188 - acc: 0.7358 - val_loss: 0.5280 - val_acc: 0.7955\n",
            "Epoch 111/400\n",
            "352/352 [==============================] - 0s 144us/step - loss: 0.5149 - acc: 0.7727 - val_loss: 0.5256 - val_acc: 0.7727\n",
            "Epoch 112/400\n",
            "352/352 [==============================] - 0s 136us/step - loss: 0.5049 - acc: 0.7812 - val_loss: 0.5258 - val_acc: 0.7386\n",
            "Epoch 113/400\n",
            "352/352 [==============================] - 0s 136us/step - loss: 0.5058 - acc: 0.7528 - val_loss: 0.5301 - val_acc: 0.7614\n",
            "Epoch 114/400\n",
            "352/352 [==============================] - 0s 135us/step - loss: 0.5225 - acc: 0.7330 - val_loss: 0.5247 - val_acc: 0.7727\n",
            "Epoch 115/400\n",
            "352/352 [==============================] - 0s 136us/step - loss: 0.5102 - acc: 0.7472 - val_loss: 0.5242 - val_acc: 0.7841\n",
            "Epoch 116/400\n",
            "352/352 [==============================] - 0s 134us/step - loss: 0.5040 - acc: 0.7642 - val_loss: 0.5478 - val_acc: 0.7500\n",
            "Epoch 117/400\n",
            "352/352 [==============================] - 0s 129us/step - loss: 0.5045 - acc: 0.7699 - val_loss: 0.5281 - val_acc: 0.7841\n",
            "Epoch 118/400\n",
            "352/352 [==============================] - 0s 133us/step - loss: 0.5146 - acc: 0.7614 - val_loss: 0.5239 - val_acc: 0.7955\n",
            "Epoch 119/400\n",
            "352/352 [==============================] - 0s 138us/step - loss: 0.5132 - acc: 0.7585 - val_loss: 0.5232 - val_acc: 0.7727\n",
            "Epoch 120/400\n",
            "352/352 [==============================] - 0s 131us/step - loss: 0.5219 - acc: 0.7330 - val_loss: 0.5361 - val_acc: 0.7955\n",
            "Epoch 121/400\n",
            "352/352 [==============================] - 0s 138us/step - loss: 0.4968 - acc: 0.7443 - val_loss: 0.5234 - val_acc: 0.7500\n",
            "Epoch 122/400\n",
            "352/352 [==============================] - 0s 142us/step - loss: 0.5129 - acc: 0.7528 - val_loss: 0.5400 - val_acc: 0.7841\n",
            "Epoch 123/400\n",
            "352/352 [==============================] - 0s 140us/step - loss: 0.4998 - acc: 0.7614 - val_loss: 0.5242 - val_acc: 0.7500\n",
            "Epoch 124/400\n",
            "352/352 [==============================] - 0s 127us/step - loss: 0.5111 - acc: 0.7330 - val_loss: 0.5281 - val_acc: 0.7727\n",
            "Epoch 125/400\n",
            "352/352 [==============================] - 0s 147us/step - loss: 0.4948 - acc: 0.7699 - val_loss: 0.5230 - val_acc: 0.7500\n",
            "Epoch 126/400\n",
            "352/352 [==============================] - 0s 139us/step - loss: 0.5029 - acc: 0.7528 - val_loss: 0.5310 - val_acc: 0.7727\n",
            "Epoch 127/400\n",
            "352/352 [==============================] - 0s 141us/step - loss: 0.5149 - acc: 0.7642 - val_loss: 0.5246 - val_acc: 0.7841\n",
            "Epoch 128/400\n",
            "352/352 [==============================] - 0s 135us/step - loss: 0.5053 - acc: 0.7472 - val_loss: 0.5241 - val_acc: 0.7500\n",
            "Epoch 129/400\n",
            "352/352 [==============================] - 0s 134us/step - loss: 0.5010 - acc: 0.7557 - val_loss: 0.5204 - val_acc: 0.7614\n",
            "Epoch 130/400\n",
            "352/352 [==============================] - 0s 138us/step - loss: 0.5003 - acc: 0.7670 - val_loss: 0.5250 - val_acc: 0.7614\n",
            "Epoch 131/400\n",
            "352/352 [==============================] - 0s 135us/step - loss: 0.5012 - acc: 0.7528 - val_loss: 0.5197 - val_acc: 0.7727\n",
            "Epoch 132/400\n",
            "352/352 [==============================] - 0s 135us/step - loss: 0.4953 - acc: 0.7585 - val_loss: 0.5192 - val_acc: 0.7727\n",
            "Epoch 133/400\n",
            "352/352 [==============================] - 0s 124us/step - loss: 0.5002 - acc: 0.7415 - val_loss: 0.5200 - val_acc: 0.7614\n",
            "Epoch 134/400\n",
            "352/352 [==============================] - 0s 134us/step - loss: 0.5066 - acc: 0.7614 - val_loss: 0.5198 - val_acc: 0.7955\n",
            "Epoch 135/400\n",
            "352/352 [==============================] - 0s 132us/step - loss: 0.5046 - acc: 0.7528 - val_loss: 0.5202 - val_acc: 0.7955\n",
            "Epoch 136/400\n",
            "352/352 [==============================] - 0s 140us/step - loss: 0.5037 - acc: 0.7614 - val_loss: 0.5191 - val_acc: 0.7614\n",
            "Epoch 137/400\n",
            "352/352 [==============================] - 0s 140us/step - loss: 0.4970 - acc: 0.7642 - val_loss: 0.5214 - val_acc: 0.7614\n",
            "Epoch 138/400\n",
            "352/352 [==============================] - 0s 137us/step - loss: 0.4972 - acc: 0.7756 - val_loss: 0.5188 - val_acc: 0.7614\n",
            "Epoch 139/400\n",
            "352/352 [==============================] - 0s 131us/step - loss: 0.4995 - acc: 0.7642 - val_loss: 0.5198 - val_acc: 0.7614\n",
            "Epoch 140/400\n",
            "352/352 [==============================] - 0s 135us/step - loss: 0.4838 - acc: 0.7841 - val_loss: 0.5383 - val_acc: 0.7727\n",
            "Epoch 141/400\n",
            "352/352 [==============================] - 0s 129us/step - loss: 0.4926 - acc: 0.7898 - val_loss: 0.5292 - val_acc: 0.7727\n",
            "Epoch 142/400\n",
            "352/352 [==============================] - 0s 147us/step - loss: 0.5042 - acc: 0.7642 - val_loss: 0.5173 - val_acc: 0.7614\n",
            "Epoch 143/400\n",
            "352/352 [==============================] - 0s 135us/step - loss: 0.4863 - acc: 0.7699 - val_loss: 0.5443 - val_acc: 0.7727\n",
            "Epoch 144/400\n",
            "352/352 [==============================] - 0s 133us/step - loss: 0.4906 - acc: 0.7699 - val_loss: 0.5145 - val_acc: 0.7727\n",
            "Epoch 145/400\n",
            "352/352 [==============================] - 0s 144us/step - loss: 0.4849 - acc: 0.7784 - val_loss: 0.5325 - val_acc: 0.7841\n",
            "Epoch 146/400\n",
            "352/352 [==============================] - 0s 132us/step - loss: 0.4968 - acc: 0.7784 - val_loss: 0.5136 - val_acc: 0.7614\n",
            "Epoch 147/400\n",
            "352/352 [==============================] - 0s 136us/step - loss: 0.4879 - acc: 0.7614 - val_loss: 0.5137 - val_acc: 0.7841\n",
            "Epoch 148/400\n",
            "352/352 [==============================] - 0s 133us/step - loss: 0.4946 - acc: 0.7727 - val_loss: 0.5445 - val_acc: 0.7727\n",
            "Epoch 149/400\n",
            "352/352 [==============================] - 0s 130us/step - loss: 0.4834 - acc: 0.7670 - val_loss: 0.5123 - val_acc: 0.7614\n",
            "Epoch 150/400\n",
            "352/352 [==============================] - 0s 139us/step - loss: 0.4884 - acc: 0.7727 - val_loss: 0.5130 - val_acc: 0.7614\n",
            "Epoch 151/400\n",
            "352/352 [==============================] - 0s 136us/step - loss: 0.4805 - acc: 0.7898 - val_loss: 0.5252 - val_acc: 0.7727\n",
            "Epoch 152/400\n",
            "352/352 [==============================] - 0s 141us/step - loss: 0.4863 - acc: 0.7784 - val_loss: 0.5176 - val_acc: 0.7841\n",
            "Epoch 153/400\n",
            "352/352 [==============================] - 0s 138us/step - loss: 0.5076 - acc: 0.7614 - val_loss: 0.5301 - val_acc: 0.7955\n",
            "Epoch 154/400\n",
            "352/352 [==============================] - 0s 129us/step - loss: 0.5131 - acc: 0.7727 - val_loss: 0.5185 - val_acc: 0.7727\n",
            "Epoch 155/400\n",
            "352/352 [==============================] - 0s 141us/step - loss: 0.4765 - acc: 0.7869 - val_loss: 0.5134 - val_acc: 0.7727\n",
            "Epoch 156/400\n",
            "352/352 [==============================] - 0s 138us/step - loss: 0.4842 - acc: 0.7699 - val_loss: 0.5126 - val_acc: 0.7727\n",
            "Epoch 157/400\n",
            "352/352 [==============================] - 0s 144us/step - loss: 0.4825 - acc: 0.7756 - val_loss: 0.5248 - val_acc: 0.7727\n",
            "Epoch 158/400\n",
            "352/352 [==============================] - 0s 137us/step - loss: 0.4950 - acc: 0.7585 - val_loss: 0.5102 - val_acc: 0.7500\n",
            "Epoch 159/400\n",
            "352/352 [==============================] - 0s 136us/step - loss: 0.4790 - acc: 0.7841 - val_loss: 0.5153 - val_acc: 0.7727\n",
            "Epoch 160/400\n",
            "352/352 [==============================] - 0s 130us/step - loss: 0.4838 - acc: 0.7614 - val_loss: 0.5403 - val_acc: 0.7727\n",
            "Epoch 161/400\n",
            "352/352 [==============================] - 0s 134us/step - loss: 0.5078 - acc: 0.7443 - val_loss: 0.5117 - val_acc: 0.7841\n",
            "Epoch 162/400\n",
            "352/352 [==============================] - 0s 138us/step - loss: 0.4943 - acc: 0.7472 - val_loss: 0.5175 - val_acc: 0.7727\n",
            "Epoch 163/400\n",
            "352/352 [==============================] - 0s 131us/step - loss: 0.4791 - acc: 0.7756 - val_loss: 0.5086 - val_acc: 0.7614\n",
            "Epoch 164/400\n",
            "352/352 [==============================] - 0s 136us/step - loss: 0.4837 - acc: 0.7841 - val_loss: 0.5257 - val_acc: 0.7955\n",
            "Epoch 165/400\n",
            "352/352 [==============================] - 0s 132us/step - loss: 0.4805 - acc: 0.7813 - val_loss: 0.5120 - val_acc: 0.7841\n",
            "Epoch 166/400\n",
            "352/352 [==============================] - 0s 141us/step - loss: 0.4761 - acc: 0.7727 - val_loss: 0.5079 - val_acc: 0.7841\n",
            "Epoch 167/400\n",
            "352/352 [==============================] - 0s 144us/step - loss: 0.4859 - acc: 0.7699 - val_loss: 0.5806 - val_acc: 0.7159\n",
            "Epoch 168/400\n",
            "352/352 [==============================] - 0s 142us/step - loss: 0.5124 - acc: 0.7699 - val_loss: 0.5120 - val_acc: 0.7841\n",
            "Epoch 169/400\n",
            "352/352 [==============================] - 0s 133us/step - loss: 0.4783 - acc: 0.7784 - val_loss: 0.5389 - val_acc: 0.7727\n",
            "Epoch 170/400\n",
            "352/352 [==============================] - 0s 138us/step - loss: 0.4942 - acc: 0.7784 - val_loss: 0.5090 - val_acc: 0.7841\n",
            "Epoch 171/400\n",
            "352/352 [==============================] - 0s 127us/step - loss: 0.4747 - acc: 0.7926 - val_loss: 0.5118 - val_acc: 0.7841\n",
            "Epoch 172/400\n",
            "352/352 [==============================] - 0s 136us/step - loss: 0.4775 - acc: 0.7784 - val_loss: 0.5068 - val_acc: 0.7841\n",
            "Epoch 173/400\n",
            "352/352 [==============================] - 0s 149us/step - loss: 0.4806 - acc: 0.7812 - val_loss: 0.5082 - val_acc: 0.7841\n",
            "Epoch 174/400\n",
            "352/352 [==============================] - 0s 136us/step - loss: 0.4771 - acc: 0.7926 - val_loss: 0.5095 - val_acc: 0.7841\n",
            "Epoch 175/400\n",
            "352/352 [==============================] - 0s 132us/step - loss: 0.4773 - acc: 0.7699 - val_loss: 0.5165 - val_acc: 0.7727\n",
            "Epoch 176/400\n",
            "352/352 [==============================] - 0s 136us/step - loss: 0.4717 - acc: 0.7926 - val_loss: 0.5242 - val_acc: 0.7841\n",
            "Epoch 177/400\n",
            "352/352 [==============================] - 0s 142us/step - loss: 0.4881 - acc: 0.7642 - val_loss: 0.5059 - val_acc: 0.7727\n",
            "Epoch 178/400\n",
            "352/352 [==============================] - 0s 138us/step - loss: 0.4733 - acc: 0.7898 - val_loss: 0.5047 - val_acc: 0.7727\n",
            "Epoch 179/400\n",
            "352/352 [==============================] - 0s 139us/step - loss: 0.4721 - acc: 0.7784 - val_loss: 0.5113 - val_acc: 0.7841\n",
            "Epoch 180/400\n",
            "352/352 [==============================] - 0s 132us/step - loss: 0.4963 - acc: 0.7500 - val_loss: 0.5044 - val_acc: 0.7727\n",
            "Epoch 181/400\n",
            "352/352 [==============================] - 0s 135us/step - loss: 0.4848 - acc: 0.7812 - val_loss: 0.5142 - val_acc: 0.7614\n",
            "Epoch 182/400\n",
            "352/352 [==============================] - 0s 133us/step - loss: 0.4829 - acc: 0.7784 - val_loss: 0.5074 - val_acc: 0.7841\n",
            "Epoch 183/400\n",
            "352/352 [==============================] - 0s 136us/step - loss: 0.4748 - acc: 0.7898 - val_loss: 0.5129 - val_acc: 0.7727\n",
            "Epoch 184/400\n",
            "352/352 [==============================] - 0s 133us/step - loss: 0.4741 - acc: 0.7784 - val_loss: 0.5030 - val_acc: 0.7841\n",
            "Epoch 185/400\n",
            "352/352 [==============================] - 0s 135us/step - loss: 0.4647 - acc: 0.8040 - val_loss: 0.5030 - val_acc: 0.7841\n",
            "Epoch 186/400\n",
            "352/352 [==============================] - 0s 129us/step - loss: 0.4944 - acc: 0.7528 - val_loss: 0.5215 - val_acc: 0.7727\n",
            "Epoch 187/400\n",
            "352/352 [==============================] - 0s 138us/step - loss: 0.4740 - acc: 0.7926 - val_loss: 0.5027 - val_acc: 0.7727\n",
            "Epoch 188/400\n",
            "352/352 [==============================] - 0s 133us/step - loss: 0.4681 - acc: 0.7898 - val_loss: 0.5059 - val_acc: 0.7841\n",
            "Epoch 189/400\n",
            "352/352 [==============================] - 0s 130us/step - loss: 0.4646 - acc: 0.7955 - val_loss: 0.5242 - val_acc: 0.7841\n",
            "Epoch 190/400\n",
            "352/352 [==============================] - 0s 136us/step - loss: 0.4726 - acc: 0.7841 - val_loss: 0.5025 - val_acc: 0.7841\n",
            "Epoch 191/400\n",
            "352/352 [==============================] - 0s 130us/step - loss: 0.4715 - acc: 0.7898 - val_loss: 0.5041 - val_acc: 0.7841\n",
            "Epoch 192/400\n",
            "352/352 [==============================] - 0s 140us/step - loss: 0.4639 - acc: 0.7955 - val_loss: 0.5039 - val_acc: 0.7841\n",
            "Epoch 193/400\n",
            "352/352 [==============================] - 0s 135us/step - loss: 0.4908 - acc: 0.7642 - val_loss: 0.5026 - val_acc: 0.7841\n",
            "Epoch 194/400\n",
            "352/352 [==============================] - 0s 150us/step - loss: 0.4679 - acc: 0.7926 - val_loss: 0.5059 - val_acc: 0.7841\n",
            "Epoch 195/400\n",
            "352/352 [==============================] - 0s 133us/step - loss: 0.4607 - acc: 0.8040 - val_loss: 0.5013 - val_acc: 0.7727\n",
            "Epoch 196/400\n",
            "352/352 [==============================] - 0s 131us/step - loss: 0.4706 - acc: 0.7784 - val_loss: 0.5044 - val_acc: 0.7841\n",
            "Epoch 197/400\n",
            "352/352 [==============================] - 0s 133us/step - loss: 0.4692 - acc: 0.7642 - val_loss: 0.5016 - val_acc: 0.7841\n",
            "Epoch 198/400\n",
            "352/352 [==============================] - 0s 129us/step - loss: 0.4586 - acc: 0.7983 - val_loss: 0.5019 - val_acc: 0.7614\n",
            "Epoch 199/400\n",
            "352/352 [==============================] - 0s 140us/step - loss: 0.4850 - acc: 0.7756 - val_loss: 0.4998 - val_acc: 0.7955\n",
            "Epoch 200/400\n",
            "352/352 [==============================] - 0s 139us/step - loss: 0.4639 - acc: 0.7926 - val_loss: 0.4997 - val_acc: 0.7955\n",
            "Epoch 201/400\n",
            "352/352 [==============================] - 0s 129us/step - loss: 0.4649 - acc: 0.7983 - val_loss: 0.5048 - val_acc: 0.7841\n",
            "Epoch 202/400\n",
            "352/352 [==============================] - 0s 137us/step - loss: 0.4686 - acc: 0.7841 - val_loss: 0.5140 - val_acc: 0.7841\n",
            "Epoch 203/400\n",
            "352/352 [==============================] - 0s 141us/step - loss: 0.4722 - acc: 0.7898 - val_loss: 0.5088 - val_acc: 0.7955\n",
            "Epoch 204/400\n",
            "352/352 [==============================] - 0s 138us/step - loss: 0.4654 - acc: 0.7955 - val_loss: 0.5005 - val_acc: 0.7841\n",
            "Epoch 205/400\n",
            "352/352 [==============================] - 0s 137us/step - loss: 0.4624 - acc: 0.7869 - val_loss: 0.4990 - val_acc: 0.7955\n",
            "Epoch 206/400\n",
            "352/352 [==============================] - 0s 139us/step - loss: 0.4622 - acc: 0.7926 - val_loss: 0.5167 - val_acc: 0.7727\n",
            "Epoch 207/400\n",
            "352/352 [==============================] - 0s 132us/step - loss: 0.4565 - acc: 0.8011 - val_loss: 0.5025 - val_acc: 0.7841\n",
            "Epoch 208/400\n",
            "352/352 [==============================] - 0s 129us/step - loss: 0.4537 - acc: 0.8011 - val_loss: 0.5021 - val_acc: 0.7841\n",
            "Epoch 209/400\n",
            "352/352 [==============================] - 0s 131us/step - loss: 0.4695 - acc: 0.8011 - val_loss: 0.5163 - val_acc: 0.7727\n",
            "Epoch 210/400\n",
            "352/352 [==============================] - 0s 134us/step - loss: 0.4623 - acc: 0.8097 - val_loss: 0.5256 - val_acc: 0.7841\n",
            "Epoch 211/400\n",
            "352/352 [==============================] - 0s 134us/step - loss: 0.4570 - acc: 0.7812 - val_loss: 0.4979 - val_acc: 0.7841\n",
            "Epoch 212/400\n",
            "352/352 [==============================] - 0s 139us/step - loss: 0.4656 - acc: 0.7983 - val_loss: 0.5154 - val_acc: 0.7727\n",
            "Epoch 213/400\n",
            "352/352 [==============================] - 0s 133us/step - loss: 0.4580 - acc: 0.7926 - val_loss: 0.5236 - val_acc: 0.7841\n",
            "Epoch 214/400\n",
            "352/352 [==============================] - 0s 132us/step - loss: 0.4642 - acc: 0.7784 - val_loss: 0.5311 - val_acc: 0.7727\n",
            "Epoch 215/400\n",
            "352/352 [==============================] - 0s 137us/step - loss: 0.4607 - acc: 0.7898 - val_loss: 0.4968 - val_acc: 0.7841\n",
            "Epoch 216/400\n",
            "352/352 [==============================] - 0s 137us/step - loss: 0.4526 - acc: 0.8011 - val_loss: 0.4960 - val_acc: 0.7841\n",
            "Epoch 217/400\n",
            "352/352 [==============================] - 0s 136us/step - loss: 0.4598 - acc: 0.8040 - val_loss: 0.4980 - val_acc: 0.7955\n",
            "Epoch 218/400\n",
            "352/352 [==============================] - 0s 139us/step - loss: 0.4740 - acc: 0.7670 - val_loss: 0.5192 - val_acc: 0.7841\n",
            "Epoch 219/400\n",
            "352/352 [==============================] - 0s 129us/step - loss: 0.4498 - acc: 0.7926 - val_loss: 0.5101 - val_acc: 0.7955\n",
            "Epoch 220/400\n",
            "352/352 [==============================] - 0s 132us/step - loss: 0.4691 - acc: 0.7869 - val_loss: 0.4980 - val_acc: 0.7955\n",
            "Epoch 221/400\n",
            "352/352 [==============================] - 0s 131us/step - loss: 0.4505 - acc: 0.7955 - val_loss: 0.5120 - val_acc: 0.7841\n",
            "Epoch 222/400\n",
            "352/352 [==============================] - 0s 133us/step - loss: 0.4512 - acc: 0.7898 - val_loss: 0.4966 - val_acc: 0.7727\n",
            "Epoch 223/400\n",
            "352/352 [==============================] - 0s 138us/step - loss: 0.4472 - acc: 0.8040 - val_loss: 0.4954 - val_acc: 0.8068\n",
            "Epoch 224/400\n",
            "352/352 [==============================] - 0s 131us/step - loss: 0.4484 - acc: 0.7955 - val_loss: 0.5080 - val_acc: 0.7955\n",
            "Epoch 225/400\n",
            "352/352 [==============================] - 0s 134us/step - loss: 0.4470 - acc: 0.8153 - val_loss: 0.5108 - val_acc: 0.7955\n",
            "Epoch 226/400\n",
            "352/352 [==============================] - 0s 137us/step - loss: 0.4459 - acc: 0.8153 - val_loss: 0.5363 - val_acc: 0.7500\n",
            "Epoch 227/400\n",
            "352/352 [==============================] - 0s 134us/step - loss: 0.4641 - acc: 0.7756 - val_loss: 0.4955 - val_acc: 0.7727\n",
            "Epoch 228/400\n",
            "352/352 [==============================] - 0s 133us/step - loss: 0.4485 - acc: 0.8210 - val_loss: 0.5088 - val_acc: 0.7955\n",
            "Epoch 229/400\n",
            "352/352 [==============================] - 0s 128us/step - loss: 0.4465 - acc: 0.7983 - val_loss: 0.5580 - val_acc: 0.7273\n",
            "Epoch 230/400\n",
            "352/352 [==============================] - 0s 137us/step - loss: 0.4795 - acc: 0.7841 - val_loss: 0.5194 - val_acc: 0.7841\n",
            "Epoch 231/400\n",
            "352/352 [==============================] - 0s 138us/step - loss: 0.4484 - acc: 0.8040 - val_loss: 0.5157 - val_acc: 0.7841\n",
            "Epoch 232/400\n",
            "352/352 [==============================] - 0s 128us/step - loss: 0.4515 - acc: 0.7955 - val_loss: 0.5000 - val_acc: 0.7955\n",
            "Epoch 233/400\n",
            "352/352 [==============================] - 0s 133us/step - loss: 0.4468 - acc: 0.8040 - val_loss: 0.5033 - val_acc: 0.7955\n",
            "Epoch 234/400\n",
            "352/352 [==============================] - 0s 139us/step - loss: 0.4366 - acc: 0.8182 - val_loss: 0.4970 - val_acc: 0.7955\n",
            "Epoch 235/400\n",
            "352/352 [==============================] - 0s 130us/step - loss: 0.4533 - acc: 0.7926 - val_loss: 0.4943 - val_acc: 0.7727\n",
            "Epoch 236/400\n",
            "352/352 [==============================] - 0s 152us/step - loss: 0.4559 - acc: 0.8011 - val_loss: 0.4973 - val_acc: 0.7955\n",
            "Epoch 237/400\n",
            "352/352 [==============================] - 0s 145us/step - loss: 0.4435 - acc: 0.8097 - val_loss: 0.4930 - val_acc: 0.7955\n",
            "Epoch 238/400\n",
            "352/352 [==============================] - 0s 124us/step - loss: 0.4498 - acc: 0.8182 - val_loss: 0.5182 - val_acc: 0.7955\n",
            "Epoch 239/400\n",
            "352/352 [==============================] - 0s 135us/step - loss: 0.4486 - acc: 0.7955 - val_loss: 0.4920 - val_acc: 0.7955\n",
            "Epoch 240/400\n",
            "352/352 [==============================] - 0s 129us/step - loss: 0.4532 - acc: 0.7926 - val_loss: 0.5416 - val_acc: 0.7386\n",
            "Epoch 241/400\n",
            "352/352 [==============================] - 0s 131us/step - loss: 0.4613 - acc: 0.8011 - val_loss: 0.4938 - val_acc: 0.7841\n",
            "Epoch 242/400\n",
            "352/352 [==============================] - 0s 130us/step - loss: 0.4389 - acc: 0.7955 - val_loss: 0.4982 - val_acc: 0.8068\n",
            "Epoch 243/400\n",
            "352/352 [==============================] - 0s 145us/step - loss: 0.4397 - acc: 0.8239 - val_loss: 0.4914 - val_acc: 0.7841\n",
            "Epoch 244/400\n",
            "352/352 [==============================] - 0s 136us/step - loss: 0.4384 - acc: 0.8011 - val_loss: 0.5324 - val_acc: 0.7614\n",
            "Epoch 245/400\n",
            "352/352 [==============================] - 0s 140us/step - loss: 0.4434 - acc: 0.7955 - val_loss: 0.5051 - val_acc: 0.7841\n",
            "Epoch 246/400\n",
            "352/352 [==============================] - 0s 138us/step - loss: 0.4419 - acc: 0.8182 - val_loss: 0.4910 - val_acc: 0.7841\n",
            "Epoch 247/400\n",
            "352/352 [==============================] - 0s 137us/step - loss: 0.4379 - acc: 0.8125 - val_loss: 0.4919 - val_acc: 0.7614\n",
            "Epoch 248/400\n",
            "352/352 [==============================] - 0s 143us/step - loss: 0.4499 - acc: 0.8068 - val_loss: 0.5021 - val_acc: 0.7955\n",
            "Epoch 249/400\n",
            "352/352 [==============================] - 0s 129us/step - loss: 0.4424 - acc: 0.7955 - val_loss: 0.5068 - val_acc: 0.7841\n",
            "Epoch 250/400\n",
            "352/352 [==============================] - 0s 135us/step - loss: 0.4337 - acc: 0.8125 - val_loss: 0.5459 - val_acc: 0.7386\n",
            "Epoch 251/400\n",
            "352/352 [==============================] - 0s 133us/step - loss: 0.4577 - acc: 0.7955 - val_loss: 0.5277 - val_acc: 0.7614\n",
            "Epoch 252/400\n",
            "352/352 [==============================] - 0s 132us/step - loss: 0.4427 - acc: 0.8040 - val_loss: 0.4990 - val_acc: 0.8068\n",
            "Epoch 253/400\n",
            "352/352 [==============================] - 0s 135us/step - loss: 0.4334 - acc: 0.8125 - val_loss: 0.4891 - val_acc: 0.7955\n",
            "Epoch 254/400\n",
            "352/352 [==============================] - 0s 136us/step - loss: 0.4352 - acc: 0.8239 - val_loss: 0.4890 - val_acc: 0.7955\n",
            "Epoch 255/400\n",
            "352/352 [==============================] - 0s 139us/step - loss: 0.4511 - acc: 0.8068 - val_loss: 0.5179 - val_acc: 0.7955\n",
            "Epoch 256/400\n",
            "352/352 [==============================] - 0s 146us/step - loss: 0.4310 - acc: 0.8153 - val_loss: 0.4994 - val_acc: 0.8068\n",
            "Epoch 257/400\n",
            "352/352 [==============================] - 0s 144us/step - loss: 0.4344 - acc: 0.8182 - val_loss: 0.4879 - val_acc: 0.7955\n",
            "Epoch 258/400\n",
            "352/352 [==============================] - 0s 127us/step - loss: 0.4434 - acc: 0.8153 - val_loss: 0.4972 - val_acc: 0.8068\n",
            "Epoch 259/400\n",
            "352/352 [==============================] - 0s 135us/step - loss: 0.4743 - acc: 0.7699 - val_loss: 0.4999 - val_acc: 0.7841\n",
            "Epoch 260/400\n",
            "352/352 [==============================] - 0s 138us/step - loss: 0.4350 - acc: 0.8040 - val_loss: 0.4893 - val_acc: 0.7955\n",
            "Epoch 261/400\n",
            "352/352 [==============================] - 0s 139us/step - loss: 0.4306 - acc: 0.8153 - val_loss: 0.4875 - val_acc: 0.7955\n",
            "Epoch 262/400\n",
            "352/352 [==============================] - 0s 129us/step - loss: 0.4303 - acc: 0.8068 - val_loss: 0.5050 - val_acc: 0.7841\n",
            "Epoch 263/400\n",
            "352/352 [==============================] - 0s 135us/step - loss: 0.4340 - acc: 0.7955 - val_loss: 0.4908 - val_acc: 0.7955\n",
            "Epoch 264/400\n",
            "352/352 [==============================] - 0s 144us/step - loss: 0.4328 - acc: 0.8097 - val_loss: 0.4867 - val_acc: 0.7955\n",
            "Epoch 265/400\n",
            "352/352 [==============================] - 0s 136us/step - loss: 0.4452 - acc: 0.8040 - val_loss: 0.4984 - val_acc: 0.7955\n",
            "Epoch 266/400\n",
            "352/352 [==============================] - 0s 127us/step - loss: 0.4336 - acc: 0.8182 - val_loss: 0.5024 - val_acc: 0.7841\n",
            "Epoch 267/400\n",
            "352/352 [==============================] - 0s 128us/step - loss: 0.4276 - acc: 0.8153 - val_loss: 0.4864 - val_acc: 0.7955\n",
            "Epoch 268/400\n",
            "352/352 [==============================] - 0s 135us/step - loss: 0.4377 - acc: 0.8182 - val_loss: 0.4859 - val_acc: 0.7955\n",
            "Epoch 269/400\n",
            "352/352 [==============================] - 0s 140us/step - loss: 0.4334 - acc: 0.8153 - val_loss: 0.4902 - val_acc: 0.8068\n",
            "Epoch 270/400\n",
            "352/352 [==============================] - 0s 134us/step - loss: 0.4227 - acc: 0.8210 - val_loss: 0.5595 - val_acc: 0.7159\n",
            "Epoch 271/400\n",
            "352/352 [==============================] - 0s 135us/step - loss: 0.4905 - acc: 0.7727 - val_loss: 0.4887 - val_acc: 0.7841\n",
            "Epoch 272/400\n",
            "352/352 [==============================] - 0s 153us/step - loss: 0.4641 - acc: 0.7642 - val_loss: 0.4879 - val_acc: 0.7955\n",
            "Epoch 273/400\n",
            "352/352 [==============================] - 0s 128us/step - loss: 0.4227 - acc: 0.8324 - val_loss: 0.4867 - val_acc: 0.7955\n",
            "Epoch 274/400\n",
            "352/352 [==============================] - 0s 137us/step - loss: 0.4228 - acc: 0.8295 - val_loss: 0.4837 - val_acc: 0.7955\n",
            "Epoch 275/400\n",
            "352/352 [==============================] - 0s 131us/step - loss: 0.4336 - acc: 0.8239 - val_loss: 0.4977 - val_acc: 0.7955\n",
            "Epoch 276/400\n",
            "352/352 [==============================] - 0s 132us/step - loss: 0.4295 - acc: 0.8295 - val_loss: 0.4856 - val_acc: 0.7955\n",
            "Epoch 277/400\n",
            "352/352 [==============================] - 0s 143us/step - loss: 0.4249 - acc: 0.8153 - val_loss: 0.5006 - val_acc: 0.7955\n",
            "Epoch 278/400\n",
            "352/352 [==============================] - 0s 139us/step - loss: 0.4295 - acc: 0.8267 - val_loss: 0.4979 - val_acc: 0.7955\n",
            "Epoch 279/400\n",
            "352/352 [==============================] - 0s 132us/step - loss: 0.4251 - acc: 0.8210 - val_loss: 0.5073 - val_acc: 0.7841\n",
            "Epoch 280/400\n",
            "352/352 [==============================] - 0s 125us/step - loss: 0.4315 - acc: 0.8352 - val_loss: 0.4858 - val_acc: 0.7955\n",
            "Epoch 281/400\n",
            "352/352 [==============================] - 0s 131us/step - loss: 0.4268 - acc: 0.8068 - val_loss: 0.4829 - val_acc: 0.8068\n",
            "Epoch 282/400\n",
            "352/352 [==============================] - 0s 133us/step - loss: 0.4404 - acc: 0.7955 - val_loss: 0.5232 - val_acc: 0.7727\n",
            "Epoch 283/400\n",
            "352/352 [==============================] - 0s 123us/step - loss: 0.4241 - acc: 0.8125 - val_loss: 0.5104 - val_acc: 0.7955\n",
            "Epoch 284/400\n",
            "352/352 [==============================] - 0s 131us/step - loss: 0.4343 - acc: 0.8097 - val_loss: 0.4912 - val_acc: 0.8068\n",
            "Epoch 285/400\n",
            "352/352 [==============================] - 0s 128us/step - loss: 0.4200 - acc: 0.8267 - val_loss: 0.4959 - val_acc: 0.7955\n",
            "Epoch 286/400\n",
            "352/352 [==============================] - 0s 132us/step - loss: 0.4399 - acc: 0.8068 - val_loss: 0.5235 - val_acc: 0.7727\n",
            "Epoch 287/400\n",
            "352/352 [==============================] - 0s 128us/step - loss: 0.4330 - acc: 0.8011 - val_loss: 0.4867 - val_acc: 0.8068\n",
            "Epoch 288/400\n",
            "352/352 [==============================] - 0s 130us/step - loss: 0.4253 - acc: 0.8324 - val_loss: 0.5167 - val_acc: 0.7841\n",
            "Epoch 289/400\n",
            "352/352 [==============================] - 0s 132us/step - loss: 0.4199 - acc: 0.8182 - val_loss: 0.4882 - val_acc: 0.8068\n",
            "Epoch 290/400\n",
            "352/352 [==============================] - 0s 134us/step - loss: 0.4236 - acc: 0.8381 - val_loss: 0.4964 - val_acc: 0.7955\n",
            "Epoch 291/400\n",
            "352/352 [==============================] - 0s 124us/step - loss: 0.4178 - acc: 0.8153 - val_loss: 0.5027 - val_acc: 0.7955\n",
            "Epoch 292/400\n",
            "352/352 [==============================] - 0s 124us/step - loss: 0.4151 - acc: 0.8239 - val_loss: 0.4811 - val_acc: 0.7955\n",
            "Epoch 293/400\n",
            "352/352 [==============================] - 0s 130us/step - loss: 0.4419 - acc: 0.8125 - val_loss: 0.4912 - val_acc: 0.7955\n",
            "Epoch 294/400\n",
            "352/352 [==============================] - 0s 130us/step - loss: 0.4227 - acc: 0.8239 - val_loss: 0.5491 - val_acc: 0.7159\n",
            "Epoch 295/400\n",
            "352/352 [==============================] - 0s 137us/step - loss: 0.4548 - acc: 0.7983 - val_loss: 0.4807 - val_acc: 0.7841\n",
            "Epoch 296/400\n",
            "352/352 [==============================] - 0s 130us/step - loss: 0.4162 - acc: 0.8494 - val_loss: 0.4954 - val_acc: 0.7955\n",
            "Epoch 297/400\n",
            "352/352 [==============================] - 0s 139us/step - loss: 0.4105 - acc: 0.8239 - val_loss: 0.4861 - val_acc: 0.8068\n",
            "Epoch 298/400\n",
            "352/352 [==============================] - 0s 131us/step - loss: 0.4208 - acc: 0.8068 - val_loss: 0.4796 - val_acc: 0.7955\n",
            "Epoch 299/400\n",
            "352/352 [==============================] - 0s 143us/step - loss: 0.4212 - acc: 0.8210 - val_loss: 0.4902 - val_acc: 0.7955\n",
            "Epoch 300/400\n",
            "352/352 [==============================] - 0s 137us/step - loss: 0.4171 - acc: 0.8239 - val_loss: 0.5305 - val_acc: 0.7273\n",
            "Epoch 301/400\n",
            "352/352 [==============================] - 0s 140us/step - loss: 0.4367 - acc: 0.8068 - val_loss: 0.4787 - val_acc: 0.8068\n",
            "Epoch 302/400\n",
            "352/352 [==============================] - 0s 133us/step - loss: 0.4211 - acc: 0.8494 - val_loss: 0.5013 - val_acc: 0.7955\n",
            "Epoch 303/400\n",
            "352/352 [==============================] - 0s 141us/step - loss: 0.4061 - acc: 0.8239 - val_loss: 0.4836 - val_acc: 0.7500\n",
            "Epoch 304/400\n",
            "352/352 [==============================] - 0s 137us/step - loss: 0.4212 - acc: 0.8125 - val_loss: 0.5035 - val_acc: 0.7955\n",
            "Epoch 305/400\n",
            "352/352 [==============================] - 0s 129us/step - loss: 0.4102 - acc: 0.8239 - val_loss: 0.4862 - val_acc: 0.7955\n",
            "Epoch 306/400\n",
            "352/352 [==============================] - 0s 137us/step - loss: 0.4074 - acc: 0.8295 - val_loss: 0.4778 - val_acc: 0.7841\n",
            "Epoch 307/400\n",
            "352/352 [==============================] - 0s 140us/step - loss: 0.4214 - acc: 0.8182 - val_loss: 0.6020 - val_acc: 0.6932\n",
            "Epoch 308/400\n",
            "352/352 [==============================] - 0s 140us/step - loss: 0.4354 - acc: 0.8210 - val_loss: 0.4875 - val_acc: 0.7955\n",
            "Epoch 309/400\n",
            "352/352 [==============================] - 0s 136us/step - loss: 0.4164 - acc: 0.8210 - val_loss: 0.4775 - val_acc: 0.7955\n",
            "Epoch 310/400\n",
            "352/352 [==============================] - 0s 131us/step - loss: 0.4148 - acc: 0.8239 - val_loss: 0.4783 - val_acc: 0.8068\n",
            "Epoch 311/400\n",
            "352/352 [==============================] - 0s 126us/step - loss: 0.4209 - acc: 0.8210 - val_loss: 0.4985 - val_acc: 0.7955\n",
            "Epoch 312/400\n",
            "352/352 [==============================] - 0s 133us/step - loss: 0.4281 - acc: 0.8182 - val_loss: 0.4885 - val_acc: 0.7955\n",
            "Epoch 313/400\n",
            "352/352 [==============================] - 0s 123us/step - loss: 0.4105 - acc: 0.8210 - val_loss: 0.4943 - val_acc: 0.7955\n",
            "Epoch 314/400\n",
            "352/352 [==============================] - 0s 136us/step - loss: 0.4082 - acc: 0.8381 - val_loss: 0.4853 - val_acc: 0.7955\n",
            "Epoch 315/400\n",
            "352/352 [==============================] - 0s 129us/step - loss: 0.4301 - acc: 0.8210 - val_loss: 0.4769 - val_acc: 0.8068\n",
            "Epoch 316/400\n",
            "352/352 [==============================] - 0s 125us/step - loss: 0.4285 - acc: 0.8324 - val_loss: 0.4976 - val_acc: 0.7955\n",
            "Epoch 317/400\n",
            "352/352 [==============================] - 0s 130us/step - loss: 0.4039 - acc: 0.8097 - val_loss: 0.4791 - val_acc: 0.8182\n",
            "Epoch 318/400\n",
            "352/352 [==============================] - 0s 125us/step - loss: 0.4203 - acc: 0.8295 - val_loss: 0.4768 - val_acc: 0.7955\n",
            "Epoch 319/400\n",
            "352/352 [==============================] - 0s 135us/step - loss: 0.4118 - acc: 0.8153 - val_loss: 0.4780 - val_acc: 0.7727\n",
            "Epoch 320/400\n",
            "352/352 [==============================] - 0s 158us/step - loss: 0.4204 - acc: 0.8097 - val_loss: 0.4782 - val_acc: 0.7955\n",
            "Epoch 321/400\n",
            "352/352 [==============================] - 0s 138us/step - loss: 0.4257 - acc: 0.8153 - val_loss: 0.4766 - val_acc: 0.7955\n",
            "Epoch 322/400\n",
            "352/352 [==============================] - 0s 132us/step - loss: 0.4340 - acc: 0.8239 - val_loss: 0.5063 - val_acc: 0.7955\n",
            "Epoch 323/400\n",
            "352/352 [==============================] - 0s 126us/step - loss: 0.4149 - acc: 0.8352 - val_loss: 0.4947 - val_acc: 0.7955\n",
            "Epoch 324/400\n",
            "352/352 [==============================] - 0s 132us/step - loss: 0.4060 - acc: 0.8352 - val_loss: 0.4926 - val_acc: 0.7955\n",
            "Epoch 325/400\n",
            "352/352 [==============================] - 0s 133us/step - loss: 0.4158 - acc: 0.8153 - val_loss: 0.4795 - val_acc: 0.7614\n",
            "Epoch 326/400\n",
            "352/352 [==============================] - 0s 141us/step - loss: 0.4360 - acc: 0.8011 - val_loss: 0.4798 - val_acc: 0.8068\n",
            "Epoch 327/400\n",
            "352/352 [==============================] - 0s 136us/step - loss: 0.4158 - acc: 0.8182 - val_loss: 0.4854 - val_acc: 0.7955\n",
            "Epoch 328/400\n",
            "352/352 [==============================] - 0s 145us/step - loss: 0.4074 - acc: 0.8182 - val_loss: 0.4902 - val_acc: 0.7955\n",
            "Epoch 329/400\n",
            "352/352 [==============================] - 0s 127us/step - loss: 0.4058 - acc: 0.8295 - val_loss: 0.4849 - val_acc: 0.7955\n",
            "Epoch 330/400\n",
            "352/352 [==============================] - 0s 134us/step - loss: 0.4147 - acc: 0.8352 - val_loss: 0.5201 - val_acc: 0.7614\n",
            "Epoch 331/400\n",
            "352/352 [==============================] - 0s 143us/step - loss: 0.4176 - acc: 0.8210 - val_loss: 0.4767 - val_acc: 0.7955\n",
            "Epoch 332/400\n",
            "352/352 [==============================] - 0s 131us/step - loss: 0.4056 - acc: 0.8523 - val_loss: 0.4856 - val_acc: 0.7955\n",
            "Epoch 333/400\n",
            "352/352 [==============================] - 0s 139us/step - loss: 0.3959 - acc: 0.8438 - val_loss: 0.4750 - val_acc: 0.7955\n",
            "Epoch 334/400\n",
            "352/352 [==============================] - 0s 130us/step - loss: 0.4043 - acc: 0.8352 - val_loss: 0.4943 - val_acc: 0.7955\n",
            "Epoch 335/400\n",
            "352/352 [==============================] - 0s 142us/step - loss: 0.4278 - acc: 0.8097 - val_loss: 0.4964 - val_acc: 0.7955\n",
            "Epoch 336/400\n",
            "352/352 [==============================] - 0s 139us/step - loss: 0.3990 - acc: 0.8324 - val_loss: 0.4852 - val_acc: 0.7955\n",
            "Epoch 337/400\n",
            "352/352 [==============================] - 0s 127us/step - loss: 0.4095 - acc: 0.8210 - val_loss: 0.4803 - val_acc: 0.7955\n",
            "Epoch 338/400\n",
            "352/352 [==============================] - 0s 131us/step - loss: 0.4018 - acc: 0.8210 - val_loss: 0.4839 - val_acc: 0.7955\n",
            "Epoch 339/400\n",
            "352/352 [==============================] - 0s 136us/step - loss: 0.4003 - acc: 0.8239 - val_loss: 0.4730 - val_acc: 0.7841\n",
            "Epoch 340/400\n",
            "352/352 [==============================] - 0s 140us/step - loss: 0.4333 - acc: 0.8125 - val_loss: 0.5024 - val_acc: 0.7955\n",
            "Epoch 341/400\n",
            "352/352 [==============================] - 0s 135us/step - loss: 0.4153 - acc: 0.8352 - val_loss: 0.4876 - val_acc: 0.7955\n",
            "Epoch 342/400\n",
            "352/352 [==============================] - 0s 133us/step - loss: 0.4041 - acc: 0.8381 - val_loss: 0.4924 - val_acc: 0.7955\n",
            "Epoch 343/400\n",
            "352/352 [==============================] - 0s 134us/step - loss: 0.4027 - acc: 0.8324 - val_loss: 0.5335 - val_acc: 0.7500\n",
            "Epoch 344/400\n",
            "352/352 [==============================] - 0s 134us/step - loss: 0.4119 - acc: 0.8381 - val_loss: 0.5104 - val_acc: 0.7727\n",
            "Epoch 345/400\n",
            "352/352 [==============================] - 0s 134us/step - loss: 0.3972 - acc: 0.8267 - val_loss: 0.4744 - val_acc: 0.7727\n",
            "Epoch 346/400\n",
            "352/352 [==============================] - 0s 146us/step - loss: 0.4121 - acc: 0.8295 - val_loss: 0.4953 - val_acc: 0.7955\n",
            "Epoch 347/400\n",
            "352/352 [==============================] - 0s 135us/step - loss: 0.3975 - acc: 0.8409 - val_loss: 0.4791 - val_acc: 0.7955\n",
            "Epoch 348/400\n",
            "352/352 [==============================] - 0s 148us/step - loss: 0.4353 - acc: 0.8040 - val_loss: 0.4840 - val_acc: 0.7955\n",
            "Epoch 349/400\n",
            "352/352 [==============================] - 0s 133us/step - loss: 0.4072 - acc: 0.8210 - val_loss: 0.5107 - val_acc: 0.7727\n",
            "Epoch 350/400\n",
            "352/352 [==============================] - 0s 132us/step - loss: 0.3994 - acc: 0.8267 - val_loss: 0.4815 - val_acc: 0.7955\n",
            "Epoch 351/400\n",
            "352/352 [==============================] - 0s 136us/step - loss: 0.4109 - acc: 0.8324 - val_loss: 0.4720 - val_acc: 0.8068\n",
            "Epoch 352/400\n",
            "352/352 [==============================] - 0s 132us/step - loss: 0.3929 - acc: 0.8381 - val_loss: 0.4768 - val_acc: 0.8068\n",
            "Epoch 353/400\n",
            "352/352 [==============================] - 0s 134us/step - loss: 0.4081 - acc: 0.8239 - val_loss: 0.5303 - val_acc: 0.7500\n",
            "Epoch 354/400\n",
            "352/352 [==============================] - 0s 136us/step - loss: 0.4035 - acc: 0.8324 - val_loss: 0.5292 - val_acc: 0.7500\n",
            "Epoch 355/400\n",
            "352/352 [==============================] - 0s 136us/step - loss: 0.4038 - acc: 0.8295 - val_loss: 0.4708 - val_acc: 0.7955\n",
            "Epoch 356/400\n",
            "352/352 [==============================] - 0s 138us/step - loss: 0.4030 - acc: 0.8267 - val_loss: 0.4771 - val_acc: 0.7955\n",
            "Epoch 357/400\n",
            "352/352 [==============================] - 0s 135us/step - loss: 0.4121 - acc: 0.8381 - val_loss: 0.4979 - val_acc: 0.7841\n",
            "Epoch 358/400\n",
            "352/352 [==============================] - 0s 139us/step - loss: 0.3988 - acc: 0.8324 - val_loss: 0.4714 - val_acc: 0.7955\n",
            "Epoch 359/400\n",
            "352/352 [==============================] - 0s 133us/step - loss: 0.3964 - acc: 0.8352 - val_loss: 0.5702 - val_acc: 0.7159\n",
            "Epoch 360/400\n",
            "352/352 [==============================] - 0s 136us/step - loss: 0.4141 - acc: 0.8295 - val_loss: 0.4728 - val_acc: 0.8068\n",
            "Epoch 361/400\n",
            "352/352 [==============================] - 0s 161us/step - loss: 0.3986 - acc: 0.8295 - val_loss: 0.4875 - val_acc: 0.7955\n",
            "Epoch 362/400\n",
            "352/352 [==============================] - 0s 135us/step - loss: 0.4170 - acc: 0.8239 - val_loss: 0.4738 - val_acc: 0.7727\n",
            "Epoch 363/400\n",
            "352/352 [==============================] - 0s 123us/step - loss: 0.3936 - acc: 0.8551 - val_loss: 0.4918 - val_acc: 0.7955\n",
            "Epoch 364/400\n",
            "352/352 [==============================] - 0s 131us/step - loss: 0.4016 - acc: 0.8324 - val_loss: 0.5573 - val_acc: 0.7159\n",
            "Epoch 365/400\n",
            "352/352 [==============================] - 0s 132us/step - loss: 0.4145 - acc: 0.8210 - val_loss: 0.4836 - val_acc: 0.7955\n",
            "Epoch 366/400\n",
            "352/352 [==============================] - 0s 141us/step - loss: 0.3877 - acc: 0.8437 - val_loss: 0.4982 - val_acc: 0.7841\n",
            "Epoch 367/400\n",
            "352/352 [==============================] - 0s 136us/step - loss: 0.3900 - acc: 0.8324 - val_loss: 0.4733 - val_acc: 0.8068\n",
            "Epoch 368/400\n",
            "352/352 [==============================] - 0s 140us/step - loss: 0.4118 - acc: 0.8324 - val_loss: 0.4748 - val_acc: 0.7841\n",
            "Epoch 369/400\n",
            "352/352 [==============================] - 0s 143us/step - loss: 0.3980 - acc: 0.8295 - val_loss: 0.4762 - val_acc: 0.7955\n",
            "Epoch 370/400\n",
            "352/352 [==============================] - 0s 136us/step - loss: 0.4144 - acc: 0.8153 - val_loss: 0.5000 - val_acc: 0.7727\n",
            "Epoch 371/400\n",
            "352/352 [==============================] - 0s 143us/step - loss: 0.3899 - acc: 0.8381 - val_loss: 0.4698 - val_acc: 0.7955\n",
            "Epoch 372/400\n",
            "352/352 [==============================] - 0s 132us/step - loss: 0.3977 - acc: 0.8381 - val_loss: 0.4701 - val_acc: 0.7955\n",
            "Epoch 373/400\n",
            "352/352 [==============================] - 0s 132us/step - loss: 0.3890 - acc: 0.8438 - val_loss: 0.5066 - val_acc: 0.7727\n",
            "Epoch 374/400\n",
            "352/352 [==============================] - 0s 144us/step - loss: 0.3999 - acc: 0.8153 - val_loss: 0.4707 - val_acc: 0.8068\n",
            "Epoch 375/400\n",
            "352/352 [==============================] - 0s 141us/step - loss: 0.3873 - acc: 0.8409 - val_loss: 0.4698 - val_acc: 0.7841\n",
            "Epoch 376/400\n",
            "352/352 [==============================] - 0s 135us/step - loss: 0.4199 - acc: 0.8295 - val_loss: 0.4910 - val_acc: 0.7955\n",
            "Epoch 377/400\n",
            "352/352 [==============================] - 0s 134us/step - loss: 0.3887 - acc: 0.8352 - val_loss: 0.5015 - val_acc: 0.7841\n",
            "Epoch 378/400\n",
            "352/352 [==============================] - 0s 127us/step - loss: 0.3886 - acc: 0.8409 - val_loss: 0.4927 - val_acc: 0.7841\n",
            "Epoch 379/400\n",
            "352/352 [==============================] - 0s 130us/step - loss: 0.4029 - acc: 0.8466 - val_loss: 0.4700 - val_acc: 0.8068\n",
            "Epoch 380/400\n",
            "352/352 [==============================] - 0s 137us/step - loss: 0.3923 - acc: 0.8409 - val_loss: 0.5029 - val_acc: 0.7727\n",
            "Epoch 381/400\n",
            "352/352 [==============================] - 0s 142us/step - loss: 0.4210 - acc: 0.8182 - val_loss: 0.4825 - val_acc: 0.7955\n",
            "Epoch 382/400\n",
            "352/352 [==============================] - 0s 146us/step - loss: 0.3805 - acc: 0.8409 - val_loss: 0.4692 - val_acc: 0.7955\n",
            "Epoch 383/400\n",
            "352/352 [==============================] - 0s 134us/step - loss: 0.3932 - acc: 0.8494 - val_loss: 0.4829 - val_acc: 0.7955\n",
            "Epoch 384/400\n",
            "352/352 [==============================] - 0s 138us/step - loss: 0.3907 - acc: 0.8466 - val_loss: 0.4958 - val_acc: 0.7727\n",
            "Epoch 385/400\n",
            "352/352 [==============================] - 0s 132us/step - loss: 0.3895 - acc: 0.8324 - val_loss: 0.5121 - val_acc: 0.7727\n",
            "Epoch 386/400\n",
            "352/352 [==============================] - 0s 140us/step - loss: 0.4305 - acc: 0.8210 - val_loss: 0.4669 - val_acc: 0.7955\n",
            "Epoch 387/400\n",
            "352/352 [==============================] - 0s 136us/step - loss: 0.3902 - acc: 0.8381 - val_loss: 0.4910 - val_acc: 0.7841\n",
            "Epoch 388/400\n",
            "352/352 [==============================] - 0s 133us/step - loss: 0.3879 - acc: 0.8494 - val_loss: 0.4841 - val_acc: 0.7955\n",
            "Epoch 389/400\n",
            "352/352 [==============================] - 0s 132us/step - loss: 0.3874 - acc: 0.8466 - val_loss: 0.4936 - val_acc: 0.7841\n",
            "Epoch 390/400\n",
            "352/352 [==============================] - 0s 145us/step - loss: 0.3852 - acc: 0.8523 - val_loss: 0.4817 - val_acc: 0.7955\n",
            "Epoch 391/400\n",
            "352/352 [==============================] - 0s 139us/step - loss: 0.3808 - acc: 0.8352 - val_loss: 0.4655 - val_acc: 0.8068\n",
            "Epoch 392/400\n",
            "352/352 [==============================] - 0s 134us/step - loss: 0.4012 - acc: 0.8239 - val_loss: 0.4869 - val_acc: 0.7841\n",
            "Epoch 393/400\n",
            "352/352 [==============================] - 0s 130us/step - loss: 0.3867 - acc: 0.8438 - val_loss: 0.4812 - val_acc: 0.7955\n",
            "Epoch 394/400\n",
            "352/352 [==============================] - 0s 135us/step - loss: 0.3844 - acc: 0.8381 - val_loss: 0.5225 - val_acc: 0.7727\n",
            "Epoch 395/400\n",
            "352/352 [==============================] - 0s 145us/step - loss: 0.3926 - acc: 0.8267 - val_loss: 0.4639 - val_acc: 0.7955\n",
            "Epoch 396/400\n",
            "352/352 [==============================] - 0s 146us/step - loss: 0.4037 - acc: 0.8352 - val_loss: 0.5114 - val_acc: 0.7727\n",
            "Epoch 397/400\n",
            "352/352 [==============================] - 0s 136us/step - loss: 0.4027 - acc: 0.8352 - val_loss: 0.4660 - val_acc: 0.8068\n",
            "Epoch 398/400\n",
            "352/352 [==============================] - 0s 151us/step - loss: 0.3881 - acc: 0.8523 - val_loss: 0.4747 - val_acc: 0.7955\n",
            "Epoch 399/400\n",
            "352/352 [==============================] - 0s 138us/step - loss: 0.3940 - acc: 0.8352 - val_loss: 0.4880 - val_acc: 0.7841\n",
            "Epoch 400/400\n",
            "352/352 [==============================] - 0s 128us/step - loss: 0.3842 - acc: 0.8381 - val_loss: 0.4696 - val_acc: 0.7727\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f02ddda3e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "3ee54be8-0998-46f0-bbac-ac4df444cca8",
        "id": "wQsQGMefWTEA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "729/729 [==============================] - 0s 51us/step\n",
            "Test score: 0.5609426110860252\n",
            "Test accuracy: 0.7338820317318083\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}